<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Xunzhe Zhou's Homepage</title>
    <meta name="author" content="Xunzhe Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
    <style>
      body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        transition: background-color 0.3s ease, color 0.3s ease;
      }
      body.light-mode {
        background-color: #FFFFFF;
        color: #000;
      }
      body.dark-mode {
        background-color: #1F1F1F;
        color: #FFFFFF;
      }
      .name {
        text-align: center;
        font-size: xx-large;
        margin-bottom: 0;
      }
      table {
        width: 100%;
        max-width: 830px;
        border: 0;
        border-spacing: 0;
        border-collapse: separate;
        margin: auto;
      }
      td {
        padding: 2.5%;
      }
      img {
        width: 100%;
        max-width: 100%;
        object-fit: cover;
      }
      .hoverZoomLink {
        transition: transform 0.2s;
      }
      .hoverZoomLink:hover {
        transform: scale(1.04);
      }
      a {
        color: #007BFF;
        text-decoration: none;
        transition: color 0.3s ease;
      }
      body.dark-mode a {
        color: #80CFFF;
      }
      a:hover {
        animation: link-hover 0.3s ease forwards;
      }
      @keyframes link-hover {
        from {
          color: inherit;
          text-decoration: underline;
        }
        to {
          color: #f09228;
          text-decoration: underline;
        }
      }
      body.dark-mode a:hover {
        animation: link-hover-dark 0.3s ease forwards;
      }

      @keyframes link-hover-dark {
        from {
          color: inherit;
          text-decoration: underline;
        }
        to {
          color: #FFD700;
          text-decoration: underline;
        }
      }
      .news, .research, .publications, .projects {
        padding: 14px;
      }
      .footer {
        font-size: small;
        margin-top: 20px;
      }
      .theme-toggle {
        position: fixed;
        top: 10px;
        right: 10px;
        background-color: transparent;
        border: none;
        cursor: pointer;
      }
      .theme-toggle img {
        width: 40px;
        height: 40px;
        transition: transform 0.3s ease;
      }
    </style>
  </head>
  <body>
    <button class="theme-toggle" id="theme-toggle">
      <img src="assets/icon/sun.svg" id="theme-icon" alt="Toggle Theme">
    </button>
    <table>
      <tr>
        <td>
          <table>
            <tr>
              <td style="width: 60%; vertical-align: middle; text-align: justify;">
                <p class="name">Xunzhe Zhou</p>
                <p>
                  I am an undergraduate student at <a href="https://www.fudan.edu.cn/en/">Fudan University</a>, pursuing my Bachelor's degree in Computer Science and Technology.
                </p>
                <p>
                  Recently, I have been an intern at <a href="https://www.comp.nus.edu.sg/">NUS</a> working with Prof. <a href="https://linsats.github.io/">Lin Shao</a> studying robotic task and motion planning. Previously, I collaborated with Prof. <a href="https://faculty.fudan.edu.cn/xyxue/zh_CN/index.htm">Xiangyang Xue</a> and Prof. <a href="https://yanweifu.github.io/">Yanwei Fu</a> on constructing a mobile manipulation system with 3D reconstruction and vision-language models. I also worked with Prof. <a href="http://kw.fudan.edu.cn/people/xiaoyanghua/">Yanghua Xiao</a> on studying LLMs' real-world complex instruction following capabilities, and with Prof. <a href="https://faculty.fudan.edu.cn/syleng/zh_CN/index.htm">Siyang Leng</a> on nonlinear dynamical systems control.
                </p>
                <p>
                  I also had a wonderful semester at the <a href="https://www.berkeley.edu/">UC Berkeley</a> during fall 2023, where I studied reinforcement learning, deep learning, optimization models, and artificial intelligence, with GPA 4.00 / 4.00.
                </p>
                <p style="text-align:center">
                  <a href="mailto:xunzhe_zhou@outlook.com">Email</a> &nbsp;/&nbsp;
                  <a href="./assets/files/cv/CV_202410.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Zhouxunzhe">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=TJDarAwAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/xunzhe-zhou/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://x.com/XunzheZhou">Twitter</a>
                </p>
              </td>
              <td style="width: 40%; max-width: 40%;">
                <a href="assets/img/profile/berkeley.jpg">
                  <img alt="profile photo" src="assets/img/profile/berkeley.jpg" style="width:100%;max-width:100%;object-fit: cover;" class="hoverZoomLink">
                </a>
              </td>
            </tr>
          </table>

          <div class="news" style="text-align: justify;">
            <h2>News</h2>
            <p>[Dec. 2023] Our paper <strong>CELLO</strong> is accepted by AAAI 2024!</p>
          </div>

          <div class="research" style="text-align: justify;">
            <h2>Research</h2>
            <p>
              My ultimate goal is to enable robots to assist human with daily task automatically. 
            </p>
            <p>
              My current research interests focus on leveraging existing foundational models to assist robots with perception and decision-making in the physical world. Specifically, <br>1) I aim to develop a general perception system to understand complex and dynamical environments; <br>2) I seek to create a general decision-making systems for real-world long-term task planning.
            </p>
            <p>
              In the future, I plan to continue my research work with a unified lifelong learning system by <br>1) developing a data collection system in different scenarios with various tasks, <br>2) constructing a universal learning system for heterogeneous robots to learn unified skills, and <br>3) building a collaborative system to enable real robots complete daily task more efficiently.
            </p>
          </div>

          <div class="publications" style="text-align: justify;">
            <h2>Publications</h2>
            <p>
              <!-- Representative papers are <span class="highlight">highlighted</span>.  -->
              * denotes equal contribution.
            </p>
            <table>
              <!-- <tr bgcolor="#ffffd0"> -->
              <tr>
                <td style="width: 30%; vertical-align: middle;">
                  <img src="assets/img/teaser/2024_emos.gif" alt="RC" width="200">
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <a><strong><span class="papertitle">EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents</span></strong></a><br>
                  <u><strong>Xunzhe Zhou*</strong></u>, J. Chen*, C. Yu*, T. Xu, Y. Mu, M. Hu, W. Shao, Y. Wang, G. Li, L. Shao<br>
                  <em>In submission</em>, 2024<br>
                  <!-- <a href="assets/abstract/2024_rc.txt">abstract</a> -->
                  <p>
                    In the the real-world robot environment, the capability of the agent in MAS is tied to the physical composition of the robot. 
                    We introduced a multi-agent framework EMOS to improve the collaboration among heterogeneous robots with varying embodiment capabilities.
                    To evaluate how well our MAS performs, we designed Habitat-MAS benchmark, including four tasks: 1) navigation, 2) perception, 3) manipulation, and 4) comprehensive multi-floor object rearrangement.
                  </p>
                </td>
              </tr>
              <tr>
                <td style="width: 30%; vertical-align: middle;">
                  <a href="https://abbey4799.github.io/publication/cello/"><img src="./assets/img/teaser/2023_cello.png" alt="CELLO" width="200"></a>
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <a href="https://abbey4799.github.io/publication/cello/"><strong><span class="papertitle">Can Large Language Models Understand Real-World Complex Instructions?</span></strong></a><br>
                  Q. He, J. Zeng, W. Huang, L. Chen, J. Xiao, Q. He, <u><strong>Xunzhe Zhou</strong></u>, J. Liang, Y. Xiao<br>
                  <em>AAAI</em>, 2024<br>
                  <a href="https://abbey4799.github.io/publication/cello/">project page</a> / <a href="assets/abstract/2023_cello.txt">abstract</a> / <a href="./assets/files/paper/cello.pdf">paper</a> / <a href="https://github.com/Abbey4799/CELLO">code</a> / <a href="assets/bibtex/2023_cello.bib">bibtex</a> / <a href="https://underline.io/lecture/92662-can-large-language-models-understand-real-world-complex-instructionsquestion">video</a>
                  <p>
                    Current LLMs often ignore semantic constraints, generate incorrect formats, violate length or count constraints, and be unfaithful to input text.
                    We propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions. We design a real-world dataset carefully crafted by human experts with 566 samples and 9 tasks. We also established 4 criteria and corresponding metrics and compared 18 Chinese-oriented models and 15 English-oriented models.
                  </p>
                </td>
              </tr>
              <tr>
                <td style="width: 30%; vertical-align: middle;">
                  <img src="assets/img/teaser/2024_rc.png" alt="RC" width="200">
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <a><strong><span class="papertitle">Reservoir Computing as Digital Twins for Controlling Nonlinear Dynamical Systems</span></strong></a><br>
                  <u><strong>Xunzhe Zhou*</strong></u>, R. Cao*, J. Hou, C. Guan, S. Leng<br>
                  <em>In submission</em>, 2023<br>
                  <p>
                    It is difficult to control a chaotic system without knowing its differential equation and traditional control strategies require continuous adjustment with various parameters.
                    To address this issue, we leveraged Echo State Network as the digital twin to predict and control the behavior of chaotic systems. 
                    We evaluated our model performance on 3 chaotic systems and 3 control strategies collectively. 
                    And we conducted extensive experiments to validate the prediction accuracy, control efficiency, and noise robustness of our model.</p>
                </td>
              </tr>
            </table>
          </div>

          <div class="projects" style="text-align: justify;">
            <h2>Selected Projects</h2>
            <table>
                <tr>
                  <td style="width: 30%; vertical-align: middle;">
                    <a href="./assets/index/tamma.html"><img src="assets/img/teaser/2024_tamma.gif" alt="TAMMA" width="200"></a>
                  </td>
                  <td style="width: 70%; vertical-align: middle;">
                    <a href="./assets/index/tamma.html"><strong><span class="papertitle">Mobile Manipulation Based on Semantic 3D Reconstruction and VLMs</span></strong></a><br>
                  J. Hou, <u><strong>Xunzhe Zhou</strong></u>, T. Pan, J. Zhang, S. Li, J. Lin, K. Wang, J. Huang, Y. Fu, X. Xue<br>
                  <em>Construct a mobile manipulation and task planning system</em><br>
                  <a href="./assets/index/tamma.html">project page</a>/ <a>code(soon)</a>
                  <p>
                    Aiming to build a service robot with generalizability in daily life scenarios, we constructed a mobile manipulation system with a robot assembled with Franka Panda arm and Hermes mobile base. 
                    In this project, I was responsible for implementing 1) semantic grasping pose estimation, 2) semantic mobile base navigation, and 3) hierarchical task planning with 3D reconstruction and VLMs. The follow-up work: <em>TaMMa</em> (Hou et al.) was accepted by <em>CoRL 2024</em>.
                  </p>
                  </td>
              </tr>
              <tr>
                  <td style="width: 30%; vertical-align: middle;">
                    <a href="./assets/index/vcd.html"><img src="assets/img/teaser/2024_vcd.png" alt="VCD" width="200"></a>
                  </td>
                  <td style="width: 70%; vertical-align: middle;">
                  <a href="./assets/index/vcd.html"><strong><span class="papertitle">Resolving Knowledge Conflicts in Vision-Language Models</span></strong></a><br>
                  <u><strong>Xunzhe Zhou</strong></u>, X. Li, Y. Zheng, X. Xue<br>
                  <em>Construct a small VQA dataset for evaluation, resolve with contrastive decoding</em><br>
                  <a href="./assets/index/vcd.html">project page</a> / <a href="https://github.com/Zhouxunzhe/TRUTH">code</a>
                  <p>
                    VLMs tend to perform hallucination when the image input conflicts with the LLM decoder knowledge base (common sense). 
                    To resolve this issue, we constructed a small-scale VQA dataset with images involving knowledge conflicts from the Internet or generated with DALLÂ·E 3 for validation, and evaluated 8 state-of-the-art VLMs on the dataset. 
                    We also resolved the knowledge conflicts in LLaVA-1.5 with contrastive decoding.
                  </p>
                  </td>
              </tr>
              <tr>
                  <td style="width: 30%; vertical-align: middle;">
                    <a href="./assets/index/nst.html"><img src="assets/img/teaser/2023_nst.png" alt="NST" width="200"></a>
                  </td>
                  <td style="width: 70%; vertical-align: middle;">
                    <a href="./assets/index/nst.html"><strong><span class="papertitle">Neural Style Transfer Based on Fine Tuning Vision Transformer</span></strong></a><br>
                  <u><strong>Xunzhe Zhou</strong></u>, Y. Liu, Y. Zhao, Y. Chen<br>
                  <em>UC Berkeley CS182/282A course project, Fall 2023</em><br>
                  <a href="./assets/index/nst.html">project page</a> / <a href="assets/files/paper/nst.pdf">essay</a> / <a href="https://github.com/Zhouxunzhe/NST-NeuralStyleTransfer">code</a>
                  <p>
                    To improve Neural Style Transfer, we replaced the content and style encoders of StyTr<sup>2</sup> with pre-trained ViT.
                    Restricted by practical computation limitations, we leveraged a two-stage training strategy: we first froze the pre-trained ViT, just trained the decoders. Then we wrapped LoRA to fine-tune ViT with COCO datasets for joint training.
                  </p>
                  </td>
              </tr>
            </table>
          </div>

          <div class="footer" style="text-align: justify;">
            <p style="text-align: center">Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.</p>
          </div>
        </td>
      </tr>
    </table>

    <script>
        const userPrefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
        const currentTheme = localStorage.getItem('theme') ||
          (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark-mode' : 'light-mode');
        document.documentElement.classList.add(currentTheme);
        document.documentElement.style.backgroundColor = currentTheme === 'dark-mode' ?  '#1F1F1F': '#FFFFFF';
        
        document.documentElement.style.visibility = 'hidden';
        window.addEventListener('load', () => {
          document.body.classList.add(currentTheme);
          document.documentElement.style.visibility = 'visible';
        });
  
        const themeIcon = document.getElementById('theme-icon');
        themeIcon.src = currentTheme === 'dark-mode' ? 'assets/icon/moon.svg' : 'assets/icon/sun.svg';
  
        const themeToggleButton = document.getElementById('theme-toggle');
  
        themeToggleButton.addEventListener('click', () => {
          document.body.classList.toggle('dark-mode');
          document.body.classList.toggle('light-mode');
  
          const newTheme = document.body.classList.contains('dark-mode') ? 'dark-mode' : 'light-mode';
          localStorage.setItem('theme', newTheme);
  
          themeIcon.src = newTheme === 'dark-mode' ? 'assets/icon/moon.svg' : 'assets/icon/sun.svg';
          themeIcon.style.transform = 'rotate(360deg)';
          setTimeout(() => themeIcon.style.transform = '', 300);
        });
  
        window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', event => {
          const newTheme = event.matches ? 'dark-mode' : 'light-mode';
          document.body.classList.add(newTheme);
          document.body.classList.remove(event.matches ? 'light-mode' : 'dark-mode');
          localStorage.setItem('theme', newTheme);
  
          themeIcon.src = newTheme === 'dark-mode' ? 'assets/icon/moon.svg' : 'assets/icon/sun.svg';
        });
      </script>
  </body>
</html>