<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Xunzhe Zhou's Homepage</title>
    <meta name="author" content="Xunzhe Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="assets/icon/industry_robot.svg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
    <style>
      body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        transition: background-color 0.3s ease, color 0.3s ease;
      }
      body.light-mode {
        background-color: #FFFFFF;
        color: #000;
      }
      body.dark-mode {
        background-color: #1F1F1F;
        color: #FFFFFF;
      }
      .name {
        text-align: center;
        font-size: xx-large;
        margin-bottom: 0;
      }
      table {
        width: 100%;
        max-width: 1000px;
        border: 0;
        border-spacing: 0;
        border-collapse: separate;
        margin: auto;
      }
      td {
        padding: 2.5%;
      }
      img {
        width: 100%;
        max-width: 100%;
        object-fit: cover;
      }
      .hoverZoomLink {
        transition: transform 0.2s;
      }
      .hoverZoomLink:hover {
        transform: scale(1.04);
      }
      a {
        color: #007BFF;
        text-decoration: none;
        transition: color 0.3s ease;
      }
      body.dark-mode a {
        color: #80CFFF;
      }
      body.dark-mode .highlight {
        background-color: #ffffd059;
      }
      .new {
        color: #ff0000;
      }
      body.dark-mode .new {
        color: #ff4d4d;
      }
      a:hover {
        animation: link-hover 0.3s ease forwards;
      }
      @keyframes link-hover {
        from {
          color: inherit;
          text-decoration: underline;
        }
        to {
          color: #f09228;
          text-decoration: underline;
        }
      }
      body.dark-mode a:hover {
        animation: link-hover-dark 0.3s ease forwards;
      }

      @keyframes link-hover-dark {
        from {
          color: inherit;
          text-decoration: underline;
        }
        to {
          color: #FFD700;
          text-decoration: underline;
        }
      }
      .news, .research, .publications, .projects, .experience{
        padding: 14px;
      }
      .footer {
        font-size: small;
        margin-top: 20px;
      }
      .theme-toggle {
        position: fixed;
        top: 10px;
        right: 10px;
        background-color: transparent;
        border: none;
        cursor: pointer;
      }
      .theme-toggle img {
        width: 40px;
        height: 40px;
        transition: transform 0.3s ease;
      }
      .timeline {
        margin: 0;
      }
      .event {
        display: flex;
        margin-bottom: 0px;
      }
      .event-date {
        min-width: 85px;
      }
      .event-text {
        margin-left: 5px;
        flex: 1;
      }
    </style>
  </head>
  <body>
    <button class="theme-toggle" id="theme-toggle">
      <img src="assets/icon/sun.svg" id="theme-icon" alt="Toggle Theme">
    </button>
    <table>
      <tr>
        <td>
          <table>
            <tr>
              <td style="width: 60%; vertical-align: middle; text-align: justify;">
                <p class="name">Xunzhe Zhou</p>
                <p>
                  I am an undergraduate student at <a href="https://www.fudan.edu.cn/en/">Fudan University</a>, pursuing my Bachelor's degree in Computer Science and Technology.
                </p>
                <p>
                  Recently, I have been a research assistant at <a href="https://www.comp.nus.edu.sg/">NUS</a> working with Prof. <a href="https://linsats.github.io/">Lin Shao</a> studying robotic task and motion planning. 
                  Previously, I collaborated with Prof. <a href="https://yanweifu.github.io/">Yanwei Fu</a> and Prof. <a href="https://faculty.fudan.edu.cn/xyxue/zh_CN/index.htm">Xiangyang Xue</a> on constructing a mobile manipulation system with 3D Semantic Fields and Vision-Language Models. 
                  Before that, I studied LLMs' real-world complex instruction following capabilities with Prof. <a href="http://kw.fudan.edu.cn/people/xiaoyanghua/">Yanghua Xiao</a>, 
                  and nonlinear dynamical systems control with Prof. <a href="https://faculty.fudan.edu.cn/syleng/zh_CN/index.htm">Siyang Leng</a>.
                </p>
                <p>
                  I also had a wonderful semester at the <a href="https://www.berkeley.edu/">UC Berkeley</a> during fall 2023, where I studied reinforcement learning, deep learning, optimization models, and artificial intelligence, with GPA 4.00 / 4.00.
                </p>
                <p>
                  I am excited to join <a href="https://www.shlab.org.cn/">Shanghai AI Lab</a> as a research intern!
                </p>
                <p style="text-align:center">
                  <a href="mailto:xunzhe_zhou@outlook.com">Email</a> &nbsp;/&nbsp;
                  <a href="./assets/files/cv/CV_202501.pdf">CV(2025.01)</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Zhouxunzhe">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=TJDarAwAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/xunzhe-zhou/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://x.com/XunzheZhou">Twitter</a>
                </p>
              </td>
              <td style="width: 40%; max-width: 40%;">
                <a href="assets/img/profile/la.jpg">
                  <img alt="profile photo" src="assets/img/profile/la.jpg" style="width:100%;max-width:100%;object-fit: cover;" class="hoverZoomLink">
                </a>
              </td>
            </tr>
          </table>

          <!-- News -->
          <div class="news" style="text-align: justify;">
            <h2>News</h2>
            <p>
              <div class="timeline">
                <div class="event">
                  <div class="event-date"><strong><span class="new">NEW</span></strong> [Jan. 2025]</div>
                  <div class="event-text">Our paper <strong>EMOS</strong> is accepted by ICLR 2025!</div>
                </div>
                <div class="event">
                  <div class="event-date"><strong><span class="new">NEW</span></strong> [Dec. 2024]</div>
                  <div class="event-text">Excited to join Shanghai AI Lab as a research intern!</div>
                </div>
                <div class="event">
                  <div class="event-date">[Dec. 2023]</div>
                  <div class="event-text">Our paper <strong>CELLO</strong> is accepted by AAAI 2024!</div>
                </div>
                <div class="event">
                  <div class="event-date">[Aug. 2023]</div>
                  <div class="event-text">Honored to visit UC Berkeley as an exchange student!</div>
                </div>
                <!-- <div class="event">
                  <div class="event-date">[Sep. 2021]</div>
                  <div class="event-text">Proud to transfer to Computer Science and Technology major!</div>
                </div> -->
                <div class="event">
                  <div class="event-date">[Sep. 2020]</div>
                  <div class="event-text">Excited to begin my undergraduate journey at Fudan University!</div>
                </div>
              </div>
            </p>
          </div>

          <!-- <div class="research" style="text-align: justify;">
            <h2>Research</h2>
            <p>
              My research interest is to efficiently leverage generalization and common sense knowledge of foundation models for robotic decision making and policy learning.
            </p>
          </div> -->

          <!-- Publications -->
          <div class="publications" style="text-align: justify;">
            <h2>Publications</h2>
            <p>
              <!-- Representative papers are <span class="highlight">highlighted</span>.  -->
              My research interest is to efficiently leverage generalization and common sense knowledge of foundation models for robotics decision making and policy learning.
            </p>
            <p>
              * denotes equal contribution. Representative papers are <span class="highlight">highlighted</span>.
            </p>
            <table>
              <!-- <tr class="highlight"> -->

              <!-- Bi-Adapt -->
              <tr>
                <td style="width: 30%; vertical-align: middle;">
                  <img src="assets/img/teaser/2024_biadapt.gif" alt="biadapt" width="200">
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <a><strong><span class="papertitle">Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence</span></strong></a><br>
                  Jinxian Zhou, Ruihai Wu, <u><strong>Xunzhe Zhou</strong></u>, Checheng Yu, Licheng Zhong, Lin Shao<br>
                  <p style="margin-top: 5px;">
                    <em>In submission</em>, 2024<br>
                    <a href="./assets/abstract/2024_biadapt.txt">abstract</a>
                  </p>
                  <p>
                    Bimanual manipulation tasks require collaboration, leading to a high demand for extensive training data. 
                    We introduce Bi-Adapt, a novel framework for learning bi-manual manipulation of novel categories. 
                    We leverage semantic correspondence from foundation models with generalization and few-shot adaptations using minimal data to achieves high success rates in cross-category bi-manual manipulation tasks.
                  </p>
                </td>
              </tr>

              <!-- EMOS -->
              <tr class="highlight">
                <td style="width: 30%; vertical-align: middle;">
                  <a href="https://emos-project.github.io/"><img src="assets/img/teaser/2024_emos.gif" alt="emos" width="200"></a>
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <a href="https://emos-project.github.io/"><strong><span class="papertitle">EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents</span></strong></a><br>
                  <u><strong>Xunzhe Zhou*</strong></u>, Junting Chen*, Checheng Yu*, Tianqi Xu, Yao Mu, Mengkang Hu, Wenqi Shao, Yikai Wang, Guohao Li, Lin Shao<br>
                  <p style="margin-top: 5px;">
                    <strong><em>International Conference on Learning Representations (ICLR)</em>, 2025</strong><br>
                    <a href="https://emos-project.github.io/">project page</a> / <a href="./assets/abstract/2024_emos.txt">abstract</a> / <a href="./assets/files/paper/emos.pdf">paper</a> / <a href="https://github.com/SgtVincent/EMOS">code</a> / <a href="./assets/bibtex/2024_emos.bib">bibtex</a>
                  </p>
                  <p>
                    In the the real-world robot environment, the capability of the agent in MAS is tied to the physical composition of the robot. 
                    We introduced a multi-agent framework EMOS to improve the collaboration among heterogeneous robots with varying embodiment capabilities.
                    To evaluate how well our MAS performs, we designed Habitat-MAS benchmark, including four tasks: 1) navigation, 2) perception, 3) manipulation, and 4) comprehensive multi-floor object rearrangement.
                  </p>
                </td>
              </tr>

              <!-- CELLO -->
              <tr>
                <td style="width: 30%; vertical-align: middle;">
                  <a href="https://abbey4799.github.io/publication/cello/"><img src="./assets/img/teaser/2023_cello.png" alt="CELLO" width="200"></a>
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <a href="https://abbey4799.github.io/publication/cello/"><strong><span class="papertitle">Can Large Language Models Understand Real-World Complex Instructions?</span></strong></a><br>
                  Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, <u><strong>Xunzhe Zhou</strong></u>, Lida Chen, Xintao Wang, Yuncheng Huang, Haoning Ye, Zihan Li, Shisong Chen, Yikai Zhang, Zhouhong Gu, Jiaqing Liang, Yanghua Xiao<br>
                  <p style="margin-top: 5px;">
                    <strong><em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2024</strong><br>
                    <a href="https://abbey4799.github.io/publication/cello/">project page</a> / <a href="assets/abstract/2023_cello.txt">abstract</a> / <a href="./assets/files/paper/cello.pdf">paper</a> / <a href="https://github.com/Abbey4799/CELLO">code</a> / <a href="assets/bibtex/2023_cello.bib">bibtex</a> / <a href="https://underline.io/lecture/92662-can-large-language-models-understand-real-world-complex-instructionsquestion">video</a>
                  </p>
                  <p>
                    Current LLMs often ignore semantic constraints, generate incorrect formats, violate length or count constraints, and be unfaithful to input text.
                    We propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions. We design a real-world dataset carefully crafted by human experts with 566 samples and 9 tasks. We also established 4 criteria and corresponding metrics and compared 18 Chinese-oriented models and 15 English-oriented models.
                  </p>
                </td>
              </tr>

              <!-- Reservoir Computing -->
              <tr>
                <td style="width: 30%; vertical-align: middle;">
                  <img src="assets/img/teaser/2024_rc.png" alt="RC" width="200">
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <a><strong><span class="papertitle">Reservoir Computing as Digital Twins for Controlling Nonlinear Dynamical Systems</span></strong></a><br>
                  <u><strong>Xunzhe Zhou*</strong></u>, Ruizhi Cao*, Jiawen Hou, Chun Guan, Siyang Leng<br>
                  <p style="margin-top: 5px;">
                    <em>In submission</em>, 2023<br>
                  </p>
                  <p>
                    It is difficult to control a chaotic system without knowing its differential equation and traditional control strategies require continuous adjustment with various parameters.
                    To address this issue, we leveraged Echo State Network as the digital twin to predict and control the behavior of chaotic systems. 
                    We evaluated our model performance on 3 chaotic systems and 3 control strategies collectively. 
                    And we conducted extensive experiments to validate the prediction accuracy, control efficiency, and noise robustness of our model.</p>
                </td>
              </tr>
            </table>
          </div>

          <!-- Selected Projects -->
          <div class="projects" style="text-align: justify;">
            <h2>Selected Projects</h2>
            <p></p>
            <table>

              <!-- TaMMa -->
              <tr class="highlight">
                  <td style="width: 30%; vertical-align: middle;">
                    <a href="./assets/index/tamma.html"><img src="assets/img/teaser/2024_tamma.gif" alt="TAMMA" width="200"></a>
                  </td>
                  <td style="width: 70%; vertical-align: middle;">
                    <a href="./assets/index/tamma.html"><strong><span class="papertitle">Mobile Manipulation Based on 3D Semantic Fields and VLMs</span></strong></a><br>
                    Jiawei Hou, <u><strong>Xunzhe Zhou</strong></u>, Tongying Pan, Jie Zhang, Shanshan Li, Junyu Lin, Kuanning Wang, Jingshun Huang, Yanwei Fu, Xiangyang Xue<br>
                  <p style="margin-top: 5px;">
                    <em>Construct a mobile manipulation and task planning system</em><br>
                    <a href="./assets/index/tamma.html">project page</a>/ <a>code(soon)</a>
                  </p>
                  <p>
                    Aiming to build a service robot with generalizability in daily life scenarios, we constructed a mobile manipulation system with a robot assembled with Franka Panda arm and Hermes mobile base. 
                    In this project, I was responsible for implementing 1) semantic grasping pose estimation, 2) semantic mobile base navigation, and 3) hierarchical task planning with 3D Semantic Fields and VLMs. The follow-up work: <em>TaMMa</em> (Hou et al.) was accepted by <em>CoRL 2024</em>.
                  </p>
                  </td>
              </tr>

              <!-- Knowledge Conflicts -->
              <tr>
                  <td style="width: 30%; vertical-align: middle;">
                    <a href="./assets/index/vcd.html"><img src="assets/img/teaser/2024_vcd.png" alt="VCD" width="200"></a>
                  </td>
                  <td style="width: 70%; vertical-align: middle;">
                  <a href="./assets/index/vcd.html"><strong><span class="papertitle">Resolving Knowledge Conflicts in Vision-Language Models</span></strong></a><br>
                  <u><strong>Xunzhe Zhou</strong></u>, Xu Li, Yi Zheng, Xiangyang Xue<br>
                  <p style="margin-top: 5px;">
                    <em>Construct a small VQA dataset for evaluation, resolve with contrastive decoding</em><br>
                    <a href="./assets/index/vcd.html">project page</a> / <a href="https://github.com/Zhouxunzhe/TRUTH">code</a>
                  </p>
                  <p>
                    VLMs tend to perform hallucination when the image input conflicts with the LLM decoder knowledge base (common sense). 
                    To resolve this issue, we constructed a small-scale VQA dataset with images involving knowledge conflicts from the Internet or generated with DALL·E 3 for validation, and evaluated 8 state-of-the-art VLMs on the dataset. 
                    We also resolved the knowledge conflicts in LLaVA-1.5 with contrastive decoding.
                  </p>
                  </td>
              </tr>

              <!-- Style Transfer -->
              <tr>
                  <td style="width: 30%; vertical-align: middle;">
                    <a href="./assets/index/nst.html"><img src="assets/img/teaser/2023_nst.png" alt="NST" width="200"></a>
                  </td>
                  <td style="width: 70%; vertical-align: middle;">
                    <a href="./assets/index/nst.html"><strong><span class="papertitle">Neural Style Transfer Based on Fine Tuning Vision Transformer</span></strong></a><br>
                  <u><strong>Xunzhe Zhou</strong></u>, Yisi Liu, Yujie Zhao, Yuanteng Chen<br>
                  <p style="margin-top: 5px;">
                    <em>UC Berkeley CS182/282A course project, Fall 2023</em><br>
                    <a href="./assets/index/nst.html">project page</a> / <a href="assets/files/paper/nst.pdf">essay</a> / <a href="https://github.com/Zhouxunzhe/NST-NeuralStyleTransfer">code</a>
                  </p>
                  <p>
                    To improve Neural Style Transfer, we replaced the content and style encoders of StyTr<sup>2</sup> with pre-trained ViT.
                    Restricted by practical computation limitations, we leveraged a two-stage training strategy: we first froze the pre-trained ViT, just trained the decoders. Then we wrapped LoRA to fine-tune ViT with COCO datasets for joint training.
                  </p>
                  </td>
              </tr>
            </table>
          </div>

          <!-- Experience -->
          <div class="experience" style="text-align: justify;">
            <h2>Experiences</h2>
            <!-- <p></p> -->
            <table>

              <!-- PJ Lab -->
              <tr>
                <td style="width: 30%; vertical-align: middle; background-color: #FFFFFF;">
                  <img src="assets/img/exp/pjlab.png" alt="pjlab" width="200">
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <p style="font-size: 17px; margin: 0; font-weight: bold;">
                    Shanghai AI Laboratory, China
                    <span style="float: right;">2024.12 - Present</span>
                  </p>
                  <p>Research Intern</p>
                </td>
              </tr>

              <!-- NUS -->
              <tr>
                <td style="width: 30%; vertical-align: middle; background-color: #FFFFFF;">
                  <img src="assets/img/exp/nus.png" alt="NUS" width="200">
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <p style="font-size: 17px; margin: 0; font-weight: bold;">
                    National University of Singapore, Singapore
                    <span style="float: right;">2024.07 - 2024.12</span>
                  </p>
                  <p>Research Assistant (Advisor: Prof. Lin Shao)</p>
                </td>
              </tr>

              <!-- Berkeley -->
              <tr>
                  <td style="width: 30%; vertical-align: middle; background-color: #FFFFFF;">
                    <img src="assets/img/exp/berkeley.png" alt="Berkeley" width="200">
                  </td>
                  <td style="width: 70%; vertical-align: middle;">
                    <p style="font-size: 17px; margin: 0; font-weight: bold;">
                      University of California, Berkeley, USA
                      <span style="float: right;">2023.08 - 2023.12</span>
                    </p>
                    <p>Exchange Student (GPA: 4.00 / 4.00)</p>
                  </td>
              </tr>

              <!-- Fudan -->
              <tr>
                <td style="width: 30%; vertical-align: middle; background-color: #ffffff;">
                  <img src="assets/img/exp/fudan.png" alt="Fudan" width="200">
                </td>
                <td style="width: 70%; vertical-align: middle;">
                  <p style="font-size: 17px; margin: 0; font-weight: bold;">
                    Fudan University, China
                    <span style="float: right;">2020.09 - 2025.06</span>
                  </p>
                  <p>
                    B.S. in Computer Science and Technology (2021.09 - 2025.06)<br>
                    Natural Science Experimental Class (2020.09 - 2021.06)
                  </p>
                </td>
              </tr>
            </table>
          </div>

          <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=989898&w=a&t=tt&d=n0SkOoBvZTU7xJCIILfA4KdgOkhhX4zFnKKOH93eEH8&co=ffffff&ct=000000&cmo=feae3b&cmn=37b7d1'></script> -->
        
          <div class="footer" style="text-align: justify;">
            <p style="text-align: center">Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.</p>
          </div>
        </td>
      </tr>
    </table>

    <script>
        const userPrefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
        const currentTheme = localStorage.getItem('theme') ||
          (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark-mode' : 'light-mode');
        document.documentElement.classList.add(currentTheme);
        document.documentElement.style.backgroundColor = currentTheme === 'dark-mode' ?  '#1F1F1F': '#FFFFFF';
        
        document.documentElement.style.visibility = 'hidden';
        window.addEventListener('load', () => {
          document.body.classList.add(currentTheme);
          document.documentElement.style.visibility = 'visible';
        });
  
        const themeIcon = document.getElementById('theme-icon');
        themeIcon.src = currentTheme === 'dark-mode' ? 'assets/icon/moon.svg' : 'assets/icon/sun.svg';
  
        const themeToggleButton = document.getElementById('theme-toggle');
  
        themeToggleButton.addEventListener('click', () => {
          document.body.classList.toggle('dark-mode');
          document.body.classList.toggle('light-mode');
  
          const newTheme = document.body.classList.contains('dark-mode') ? 'dark-mode' : 'light-mode';
          localStorage.setItem('theme', newTheme);
  
          themeIcon.src = newTheme === 'dark-mode' ? 'assets/icon/moon.svg' : 'assets/icon/sun.svg';
          themeIcon.style.transform = 'rotate(360deg)';
          setTimeout(() => themeIcon.style.transform = '', 300);
        });
  
        window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', event => {
          const newTheme = event.matches ? 'dark-mode' : 'light-mode';
          document.body.classList.add(newTheme);
          document.body.classList.remove(event.matches ? 'light-mode' : 'dark-mode');
          localStorage.setItem('theme', newTheme);
  
          themeIcon.src = newTheme === 'dark-mode' ? 'assets/icon/moon.svg' : 'assets/icon/sun.svg';
        });
    </script>
  </body>
</html>